{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5fdc36",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "# Data Loading & Normalising Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df9aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing: Travelite Brochure\n",
      "✅ Extracted 51 pages of text\n",
      "📊 Extracted tables from pdfplumber\n",
      "🖼️ Extracting images from: Travelite Brochure\n",
      "✅ Extracted 206 image(s) to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\extracted_images\\Travelite_Brochure\n",
      "📁 Cleaned text saved to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\\Travelite_Brochure.txt\n",
      "\n",
      "📄 Processing: Air India Security\n",
      "✅ Extracted 5 pages of text\n",
      "📊 Extracted tables from pdfplumber\n",
      "🖼️ Extracting images from: Air India Security\n",
      "✅ Extracted 283 image(s) to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\extracted_images\\Air_India_Security\n",
      "📁 Cleaned text saved to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\\Air_India_Security.txt\n",
      "\n",
      "📄 Processing: Air India Express Fees\n",
      "✅ Extracted 3 pages of text\n",
      "📊 Extracted tables from pdfplumber\n",
      "🖼️ Extracting images from: Air India Express Fees\n",
      "✅ Extracted 0 image(s) to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\extracted_images\\Air_India_Express_Fees\n",
      "📁 Cleaned text saved to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\\Air_India_Express_Fees.txt\n",
      "\n",
      "📄 Processing: IndiGo ZED Policy\n",
      "✅ Extracted 11 pages of text\n",
      "📊 Extracted tables from pdfplumber\n",
      "🖼️ Extracting images from: IndiGo ZED Policy\n",
      "✅ Extracted 11 image(s) to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\extracted_images\\IndiGo_ZED_Policy\n",
      "📁 Cleaned text saved to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\\IndiGo_ZED_Policy.txt\n",
      "\n",
      "📄 Processing: Alliance Air Baggage\n",
      "✅ Extracted 5 pages of text\n",
      "📊 Extracted tables from pdfplumber\n",
      "🖼️ Extracting images from: Alliance Air Baggage\n",
      "✅ Extracted 6 image(s) to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\extracted_images\\Alliance_Air_Baggage\n",
      "📁 Cleaned text saved to: C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\\Alliance_Air_Baggage.txt\n",
      "\n",
      "✅ Step 1A complete. All text, tables, and images extracted & saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import fitz  # This is PyMuPDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 📁 Base path\n",
    "base_data_path = r\"C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\"\n",
    "\n",
    "# 📄 Input PDFs\n",
    "pdf_paths = {\n",
    "    \"Travelite Brochure\": os.path.join(base_data_path, \"raw_docs\", \"45.pdf\"),\n",
    "    \"Air India Security\": os.path.join(base_data_path, \"raw_docs\", \"security-regulations-dangerous-goods-restricted-items.pdf\"),\n",
    "    \"Air India Express Fees\": os.path.join(base_data_path, \"raw_docs\", \"AIX-FeesandCharges-12-OCT-23.pdf\"),\n",
    "    \"IndiGo ZED Policy\": os.path.join(base_data_path, \"raw_docs\", \"ZEDPolicy.pdf\"),\n",
    "    \"Alliance Air Baggage\": os.path.join(base_data_path, \"raw_docs\", \"baggage-policy.pdf\")\n",
    "}\n",
    "\n",
    "# 📁 Output folders\n",
    "cleaned_output_dir = os.path.join(base_data_path, \"cleaned_docs\")\n",
    "images_output_dir = os.path.join(base_data_path, \"extracted_images\")\n",
    "os.makedirs(cleaned_output_dir, exist_ok=True)\n",
    "os.makedirs(images_output_dir, exist_ok=True)\n",
    "\n",
    "# 🧼 Clean text\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s.,;:!?()/-]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# 🖼️ Extract images from PDF\n",
    "def extract_images(pdf_path, pdf_name):\n",
    "    print(f\"🖼️ Extracting images from: {pdf_name}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_image_dir = os.path.join(images_output_dir, pdf_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(pdf_image_dir, exist_ok=True)\n",
    "    image_count = 0\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc[page_num].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_img = doc.extract_image(xref)\n",
    "            image_data = base_img[\"image\"]\n",
    "            ext = base_img[\"ext\"]\n",
    "            image_path = os.path.join(pdf_image_dir, f\"page{page_num+1}_img{img_index+1}.{ext}\")\n",
    "            with open(image_path, \"wb\") as f:\n",
    "                f.write(image_data)\n",
    "            image_count += 1\n",
    "\n",
    "    print(f\"✅ Extracted {image_count} image(s) to: {pdf_image_dir}\")\n",
    "    return image_count\n",
    "\n",
    "# 📄 Main extraction\n",
    "def extract_and_save(name, pdf_path):\n",
    "    try:\n",
    "        print(f\"\\n📄 Processing: {name}\")\n",
    "\n",
    "        # Step 1: Extract text\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load_and_split()\n",
    "        full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        print(f\"✅ Extracted {len(documents)} pages of text\")\n",
    "\n",
    "        # Step 2: Extract tables\n",
    "        table_text = \"\"\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                for t_index, table in enumerate(tables):\n",
    "                    table_text += f\"\\n\\n[Table from Page {i+1} - Table {t_index+1}]\\n\"\n",
    "                    for row in table:\n",
    "                        table_text += \"\\t\".join(cell or \"\" for cell in row) + \"\\n\"\n",
    "        if table_text.strip():\n",
    "            print(f\"📊 Extracted tables from pdfplumber\")\n",
    "\n",
    "        # Step 3: Extract images\n",
    "        extract_images(pdf_path, name)\n",
    "\n",
    "        # Step 4: Combine and clean\n",
    "        combined_text = full_text + \"\\n\\n\" + table_text\n",
    "        cleaned_text = clean_text(combined_text)\n",
    "\n",
    "        # Step 5: Save cleaned text\n",
    "        cleaned_file_path = os.path.join(cleaned_output_dir, f\"{name.replace(' ', '_')}.txt\")\n",
    "        with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "\n",
    "        print(f\"📁 Cleaned text saved to: {cleaned_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {name}: {e}\")\n",
    "\n",
    "# ▶️ Run\n",
    "if __name__ == \"__main__\":\n",
    "    for name, path in pdf_paths.items():\n",
    "        extract_and_save(name, path)\n",
    "\n",
    "    print(\"\\n✅ Step 1A complete. All text, tables, and images extracted & saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc8e27",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6a0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loading: Air_India_Express_Fees.txt\n",
      "📄 Loading: Air_India_Security.txt\n",
      "📄 Loading: Alliance_Air_Baggage.txt\n",
      "📄 Loading: IndiGo_ZED_Policy.txt\n",
      "📄 Loading: Travelite_Brochure.txt\n",
      "\n",
      "📦 Total chunks: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naray\\OneDrive\\Attachments\\Documents\\Python Scripts\\ipykernel_17252\\3239594239.py:38: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceEmbeddings(\n",
      "c:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Creating embeddings for 359 chunks...\n",
      "\n",
      "✅ Embeddings created for all chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 📂 Paths\n",
    "cleaned_docs_dir = r\"C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\data\\cleaned_docs\"\n",
    "\n",
    "# 🔧 Chunking settings\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# 🔁 Chunker\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 📄 Load + chunk each file\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(cleaned_docs_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(cleaned_docs_dir, filename)\n",
    "        print(f\"📄 Loading: {filename}\")\n",
    "        loader = TextLoader(path, encoding=\"utf-8\")\n",
    "        raw_docs = loader.load()\n",
    "        chunks = splitter.split_documents(raw_docs)\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"source\"] = filename\n",
    "        documents.extend(chunks)\n",
    "\n",
    "print(f\"\\n📦 Total chunks: {len(documents)}\")\n",
    "\n",
    "# 🔍 Embedding model\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "# 🧠 Generate embeddings\n",
    "texts = [doc.page_content for doc in documents]\n",
    "print(f\"🔁 Creating embeddings for {len(texts)} chunks...\")\n",
    "embeddings = hf.embed_documents(texts)\n",
    "\n",
    "print(f\"\\n✅ Embeddings created for all chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\naray\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\naray\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\naray\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\naray\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\naray\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naray\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.1\n",
      "    Uninstalling numpy-2.3.1:\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\naray\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\naray\\OneDrive\\Attachments\\Documents\\Python Scripts\\pip-uninstall-rxgr4enh'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c662304",
   "metadata": {},
   "source": [
    "## ✅ validate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2b2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def validate_embeddings(embeddings: List[List[float]], documents: List[Document], show_preview=True) -> None:\n",
    "    print(\"\\n🔎 Validating embeddings...\\n\")\n",
    "\n",
    "    # 1. Count check\n",
    "    if len(embeddings) != len(documents):\n",
    "        print(f\"❌ Count mismatch: {len(embeddings)} embeddings vs {len(documents)} documents\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"✅ Embedding count matches document count: {len(embeddings)}\")\n",
    "\n",
    "    # 2. Dimensionality check\n",
    "    vector_lengths = [len(vec) for vec in embeddings]\n",
    "    unique_dims = set(vector_lengths)\n",
    "    print(f\"📏 Unique embedding dimensions: {unique_dims}\")\n",
    "    if len(unique_dims) > 1:\n",
    "        print(\"⚠️ Warning: inconsistent embedding dimensions\")\n",
    "\n",
    "    # 3. NaN check\n",
    "    nan_count = sum(np.isnan(vec).any() for vec in embeddings)\n",
    "    print(f\"❌ Embeddings with NaN values: {nan_count}\")\n",
    "\n",
    "    # 4. All-zero vector check\n",
    "    zero_count = sum(np.allclose(vec, 0) for vec in embeddings)\n",
    "    print(f\"⚠️  Embeddings that are all zeros: {zero_count}\")\n",
    "\n",
    "    # 5. Cosine similarity sample\n",
    "    if len(embeddings) >= 2:\n",
    "        sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        print(f\"🧠 Cosine similarity between chunk 0 and 1: {sim:.4f}\")\n",
    "\n",
    "    # 6. Optional preview\n",
    "    if show_preview:\n",
    "        print(\"\\n📄 Sample Chunk Text:\")\n",
    "        print(documents[0].page_content[:300], \"...\\n\")\n",
    "        print(\"🔢 Sample Embedding Vector (first 5 dims):\")\n",
    "        print(embeddings[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e44ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Validating embeddings...\n",
      "\n",
      "✅ Embedding count matches document count: 359\n",
      "📏 Unique embedding dimensions: {768}\n",
      "❌ Embeddings with NaN values: 0\n",
      "⚠️  Embeddings that are all zeros: 0\n",
      "🧠 Cosine similarity between chunk 0 and 1: 0.6709\n",
      "\n",
      "📄 Sample Chunk Text:\n",
      "1. all fees displayed are per pax per journey (unless specified) for one-way direct \n",
      "flights including taxes (if applicable)\n",
      "2. prices may vary depending on your travel period, but will not be higher than \n",
      "those reflected in the table below.\n",
      "3. guests shall be entitled to one piece of checked baggag ...\n",
      "\n",
      "🔢 Sample Embedding Vector (first 5 dims):\n",
      "[0.034289319068193436, -0.10171958059072495, -0.013270975090563297, 0.06298941373825073, 0.02094653807580471]\n"
     ]
    }
   ],
   "source": [
    "validate_embeddings(embeddings, documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc1999",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3162e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ FAISS vector store saved successfully at:\n",
      "C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\models\\rag_vector_store\\travel_docs_index\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# ✅ Combine texts + embeddings into pairs\n",
    "text_embedding_pairs = [(doc.page_content, emb) for doc, emb in zip(documents, embeddings)]\n",
    "\n",
    "# ✅ Create FAISS index\n",
    "vectorstore = FAISS.from_embeddings(\n",
    "    text_embedding_pairs,  # List of (text, embedding_vector) tuples\n",
    "    embedding=hf           # Your HuggingFaceEmbeddings instance\n",
    ")\n",
    "\n",
    "# ✅ Save FAISS index\n",
    "faiss_index_path = r\"C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\models\\rag_vector_store\\travel_docs_index\"\n",
    "os.makedirs(faiss_index_path, exist_ok=True)\n",
    "\n",
    "vectorstore.save_local(faiss_index_path)\n",
    "\n",
    "print(f\"\\n✅ FAISS vector store saved successfully at:\\n{faiss_index_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44fb6dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top Results for: what are Dangerous Goods & Restricted Items?\n",
      "\n",
      "\n",
      "📄 Result 1:\n",
      "flammable items\n",
      "such as aerosol (any except for personal care or toiletries in limited quantities), \n",
      "📎 Metadata: {}\n",
      "\n",
      "📄 Result 2:\n",
      "(including cooking fuels and any flammable liquid fuel), gasoline, gas torches, lighter\n",
      "fluid, strik\n",
      "📎 Metadata: {}\n",
      "\n",
      "📄 Result 3:\n",
      "cartridges, hand guns, fireworks, and pistol caps\n",
      " flammable liquids and solids such as lighter refi\n",
      "📎 Metadata: {}\n",
      "\n",
      "📄 Result 4:\n",
      "security regulations\n",
      "dangerous goods  restricted items\n",
      "air india does not carry any kind of dangerou\n",
      "📎 Metadata: {}\n",
      "\n",
      "📄 Result 5:\n",
      "security regulations\n",
      "items which cannot be carried in hand baggage and even as checked -in\n",
      "baggage\n",
      "e\n",
      "📎 Metadata: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naray\\OneDrive\\Attachments\\Documents\\Python Scripts\\ipykernel_17252\\1369923456.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ✅ Path to your saved FAISS index\n",
    "faiss_index_path = r\"C:\\Users\\naray\\OneDrive\\Pictures\\Desktop\\01. My Learning\\new\\smart-travel-advisor\\models\\rag_vector_store\\travel_docs_index\"\n",
    "\n",
    "# ✅ Load the FAISS index (use embeddings=)\n",
    "vectorstore = FAISS.load_local(faiss_index_path, embeddings=hf,allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ Create retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# 🔍 Test query\n",
    "query =\"what are Dangerous Goods & Restricted Items?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 📄 Show results\n",
    "print(f\"\\n🔍 Top Results for: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n📄 Result {i}:\\n{doc.page_content[:100]}\")\n",
    "    print(\"📎 Metadata:\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa660d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\naray\\\\OneDrive\\\\Pictures\\\\Desktop\\\\01. My Learning\\\\new\\\\smart-travel-advisor\\\\models\\\\rag_vector_store\\\\travel_docs_index'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79f5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Create retriever\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029fe3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naray\\OneDrive\\Attachments\\Documents\\Python Scripts\\ipykernel_17252\\1167667919.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "# 💬 Load LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.2,\n",
    "    openai_api_key=\"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\"  # Or use os.getenv()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06cac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Create RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c757061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naray\\OneDrive\\Attachments\\Documents\\Python Scripts\\ipykernel_17252\\3401675000.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain(query)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Ask a question\n",
    "query = \"what are Dangerous Goods & Restricted Items?\"\n",
    "result = rag_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0cb5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what are Dangerous Goods & Restricted Items?',\n",
       " 'result': 'Dangerous Goods and Restricted Items are items that are considered hazardous or potentially harmful if transported on an aircraft. These items include flammable materials like aerosols, fuels, and gasoline, oxidizing materials like bleaching powder, poisonous and infectious substances like insecticides, radioactive materials, corrosives like acids and alkalis, and other dangerous articles like offensive or irritating materials. These items are restricted or prohibited from being carried on board an aircraft for safety reasons.',\n",
       " 'source_documents': [Document(id='387822d0-ee21-4a54-ba11-eb615bb40e5f', metadata={}, page_content='flammable items\\nsuch as aerosol (any except for personal care or toiletries in limited quantities), fuels\\n(including cooking fuels and any flammable liquid fuel), gasoline, gas torches, lighter\\nfluid, strike anywhere matches, turpentine and paint thinner, realistic replicas of\\nincendiaries\\n\\noxidizing materials\\nsuch as bleaching powder, peroxides\\n\\npoisonous and infectious substances\\nsuch as insecticides, weed-killers and live virus materials.\\n\\nradio-active materials\\ncorrosives\\nsuch as acids, alkalis, mercury, wet cell batteries (except those in wheelchairs), oven or\\ndrain cleaners\\n\\nother dangerous articles\\nsuch as magnetized, offensive or irritating materials. briefcases and attaché cases with\\ninstalled alarm devices.'),\n",
       "  Document(id='d88cbbbd-699b-4da4-9e3d-71e70215ccbd', metadata={}, page_content='(including cooking fuels and any flammable liquid fuel), gasoline, gas torches, lighter\\nfluid, strike anywhere matches, turpentine and paint thinner, realistic replicas of\\nincendiaries\\noxidizing materials\\nsuch as bleaching powder, peroxides\\npoisonous and infectious substances\\nsuch as insecticides, weed-killers and live virus materials.\\nradio-active materials\\ncorrosives\\nsuch as acids, alkalis, mercury, wet cell batteries (except those in wheelchairs), oven or\\ndrain cleaners\\nother dangerous articles\\nsuch as magnetized, offensive or irritating materials. briefcases and attaché cases with\\ninstalled alarm devices.'),\n",
       "  Document(id='43b3c8c6-dc67-4fae-b8be-9f145a7fdf51', metadata={}, page_content='cartridges, hand guns, fireworks, and pistol caps\\n flammable liquids and solids such as lighter refills, lighter fuel, matches, paints,\\nthinners, fire-lighters, lighters that need inverting before ignition\\n radioactive material\\n briefcases and attaché cases with installed alarm devices\\n oxidizing materials such as bleaching powder and peroxides\\n poisons and infectious substances such as insecticides, weed-killers and live\\nvirus materials\\n small oxygen cylinders for medical use and small carbon dioxide gas cylinders\\nworn by passenger for the operation of mechanical limbs\\n other dangerous articles such as magnetized materials, offensive or irritating\\nmaterials')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c703d5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ✅ Ask a question\n",
    "query = \"Do domestic flights provide free meals?\"\n",
    "result = rag_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da7a3f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do domestic flights provide free meals?',\n",
       " 'result': 'Based on the provided information, for flights with a duration of less than 75 minutes, only lite bites will be available instead of hot meals. So, it seems that domestic flights may not provide free meals, especially for shorter flights.',\n",
       " 'source_documents': [Document(id='abaf3186-8a52-400d-abea-8a7798cb5aba', metadata={}, page_content='meals sandwich included included included\\nxtra carry on bag 3 kg extra cabin bag - - included\\nxpress ahead\\npriority check-in -\\nincluded includedpriority baggage -\\npriority boarding -\\nprice - domestic 700 1,000 1,700\\nprice  international 700 1,200 2,750\\nair india express\\nfees  charges - domestic\\nadvantage add-on packs\\npre-book\\ncabin baggage\\nfee 7kg 3 kgs 5 kgs\\n free 1050 1750\\nfor flight duration less than 75 minutes, only lite bites will be available instead of hot meals.\\nadd on type airport xtra pack airport \\nxecutive pack\\nprime\\nseats extra leg room seats -\\nall seats included\\nstandard seats all standard seats included\\nmeal ready to eat (rte) included included\\nxpress ahead\\npriority check-in -\\nincludedpriority baggage -\\npriority boarding -\\nprice - domestic 700 1,700'),\n",
       "  Document(id='64a91910-ffd9-4d40-9330-10e6d3166dff', metadata={}, page_content='table from page 1 - table 1\\ntype\\txtra pack\\txperience pack\\txecutive pack\\nextra leg room seats\\t-\\t50 discount\\tincluded\\nall standard seats\\tincluded\\tincluded\\tincluded\\nhot\\tincluded\\tincluded\\tincluded\\nsandwich\\tincluded\\tincluded\\tincluded\\n3 kg extra cabin bag\\t-\\t-\\tincluded\\npriority check-in\\t-\\tincluded\\tincluded\\npriority baggage\\t-\\t\\t\\npriority boarding\\t-\\t\\t\\n\\t700\\t1,000\\t1,700\\n\\t700\\t1,200\\t2,750\\n\\n\\ntable from page 1 - table 2\\ntype\\tairport xtra pack\\tairport\\nxecutive pack\\nextra leg room seats\\t-\\tall seats included\\nall standard seats\\tincluded\\t\\nready to eat (rte)\\tincluded\\tincluded\\npriority check-in\\t-\\tincluded\\npriority baggage\\t-\\t\\npriority boarding\\t-\\t\\n\\t700\\t1,700\\n\\t700\\t2,750\\n\\n\\ntable from page 1 - table 3\\n7kg\\t3 kgs\\t5 kgs\\nfree\\t1050\\t1750'),\n",
       "  Document(id='dcf606f3-a965-46a5-aaab-b9aa2de3f66a', metadata={}, page_content='table from page 4 - table 1\\nthe travelite experience')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f4fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
